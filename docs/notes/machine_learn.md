<style>
.book{
    border: 2px solid gray;
    margin: 30px;
    padding: 20px;
    margin: 0 auto;
    text-align: center;
    background-color: rgba(0,0,0,0.1);;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    border-radius: 10px;
    transition: transform 0.3s ease;
    text-align: center;
}
.book:hover{
    transform: translateY(-5px); 
    box-shadow: 0 8px 12px rgba(0, 0, 0, 0.2); 
}
.copyright{
    background-color: rgba(0,0,0,0.1); 
    text-align: center; 
    padding: 10px 0; 
    font-size: 14px;
}
</style>

<h1 class="article-title">æœºå™¨å­¦ä¹ (ML )</h1>

<br><br>

æ•°å­¦åŸºç¡€PDF

>  ML-Math - PDF

<div class="book"
    onclick="
    var pdfContainer = document.getElementById('pdf-container');
    if (pdfContainer.style.display === 'none') {
      pdfContainer.style.display = 'block';
    } else {
      pdfContainer.style.display = 'none';
    }
    ">
  <p>ML - Math</p>
  <div class="pdf-container" id="pdf-container" style="display: none;">
    <iframe src="https://drive.google.com/file/d/1R_fta4L7-cNN1mgJHJNlXO0wIb7c0FIL/view?usp=drive_link" width="100%" height="1000"></iframe>
  </div>
</div>


## æ¦‚ç‡è¿‘ä¼¼æ­£ç¡® - PAC(Probably Approximately Correct)

æœ€é‡è¦ç†è®ºæ¨¡å‹ :
$$
P (|f(X)-y| \le \epsilon)\ge 1 - \delta
$$
å…¶ä¸­:

- $X : data$
- $f(X): å¯¹Xçš„åˆ¤æ–­$ 
- $y : çœŸå®å€¼$
- $\epsilon : ä¸€ä¸ªè¶‹äºé›¶å€¼$  
- $P : æ¦‚ç‡å€¼$
- $\delta: ä¸€ä¸ªè¶‹äºé›¶å€¼$ 

 

NFLå®šç†: å…·ä½“é—®é¢˜, å…·ä½“åˆ†æ.  

æ²¡æœ‰æœ€å¥½çš„ç®—æ³•,åªæœ‰ç›¸å¯¹è¾ƒä¼˜çš„ç®—æ³• 

( é©¬å“²è´¯å½»äººç”ŸğŸ™ƒ )

## è¯¯å·®

- Underfitting(æ¬ æ‹Ÿåˆ)
- Overfitting(è¿‡æ‹Ÿåˆ)

![image-20230925213330855](/imgs/image-20230925213330855.png) 



## ä¸‰ä¸ªå…³é”®é—®é¢˜

### 1. è¯„ä¼°æ–¹æ³•

- ç•™å‡ºæ³•(hold-out)
  > å°†dataåˆ‡åˆ†ä¸ºä¸¤ä¸ªset,è®­ç»ƒé›†å’Œæµ‹è¯•é›†(0.8:0.2 ...)
  - ä¿è¯æ•°æ®åˆ†å¸ƒä¸€è‡´æ€§(åˆ†å±‚é‡‡æ ·)
  - å¤šæ¬¡é‡å¤åˆ’åˆ†(ç™¾æ¬¡æµ‹è¯•æ±‚å‡å€¼,å»é™¤åˆ‡åˆ†æ•°æ®çš„å½±å“)
  - æœ€ç»ˆé¢„æµ‹æ¨¡å‹ä¸ºå…¨æ•°æ®è®­ç»ƒ

- $k$-foldäº¤å‰éªŒè¯æ³•(cross calidation)
  > è¿›è¡Œ$k$æ¬¡åˆ’åˆ†,å»é™¤åˆ’åˆ†æ‰°åŠ¨,å†æ ¹æ®$k$ åˆ’åˆ† dataå,å¾ªç¯è®­ç»ƒ è¿™$k$ ä¸ªé›†åˆ
  - ç•™ä¸€æ³•($k =X-1$, åˆ™å¾—åˆ° level-one-out, LOO )

- è‡ªåŠ©æ³•(bootstrap)
  > åŸºäºå¯é‡å¤é‡‡æ ·, çº¦æœ‰36.8%çš„æ•°æ®ä¸ä¼šè¢«æŠ½ä¸­
  - è®­ç»ƒé›†ä¸åŸæ ·æœ¬é›†ç›¸åŒ
  - åŒ…å¤–ä¼°è®¡(out-of-bag estimation)

$$
\lim_\limits{m\to \infty} {(1-\cfrac{1}{m})} = \cfrac{1}{e} \approx 0.368
$$

#### å‚æ•°

- ç®—æ³•çš„å‚æ•°: ä¸€èˆ¬ç”±äººè®¾å®š,ä¹Ÿç§°ä¸º â€œè¶…å‚æ•°â€
- æ¨¡å‹çš„å‚æ•°: ä¸€èˆ¬ç”±å­¦ä¹ ç¡®å®š



### 2. æ€§èƒ½åº¦é‡

#### å›å½’ä»»åŠ¡

å¸¸ç”¨å‡æ–¹è¯¯å·®Err : 
$$
E(f,X) = \cfrac{1}{2m} \displaystyle \sum\limits_{i=1}^{m}(f(x_i)-y_i)^2
$$

#### åˆ†ç±»ä»»åŠ¡

- é”™è¯¯ç‡Err:

$E(f,X) = \cfrac{1}{m} \displaystyle \sum \limits_{i=1}^{m} I \cdot (f(x_i)\ne y_i)$



- ç²¾åº¦Acc:

$A(f,X) = \cfrac{1}{m}\displaystyle \sum \limits_{i=1}^{m}  I \cdot (f(x_i) = y_i)$

$= 1-E(f,X)$



 ![image-20230925224501316](/imgs/image-20230925224501316.png)

- æŸ¥å‡†ç‡: $P=\cfrac{TP}{TP+FP}$
- æŸ¥å…¨ç‡: $R=\cfrac{TP}{TP+FN}$
- $F_1$åº¦é‡: $\cfrac{1}{F_1} = \cfrac{1}{2} \cdot (\cfrac{1}{P} + \cfrac{1}{R})$
- $F_{\beta}$åº¦é‡:

$\cfrac{1}{F_{\beta}} = \cfrac{1}{1+{\beta}^2} \cdot \left(\cfrac{1}{P} + \cfrac{{\beta}^2}{R}\right)$

$\text{å½“} \begin{cases}
\beta < 1, & \text{æŸ¥å‡†ç‡å½±å“æ›´å¤§} \\
\beta > 1, & \text{æŸ¥å…¨ç‡å½±å“æ›´å¤§}
\end{cases}$

###  3. æ¯”è¾ƒæ£€éªŒ

> ç»Ÿè®¡å‡è®¾æ£€éªŒä¸ºå­¦ä¹ å™¨æ€§èƒ½æ¯”è¾ƒæä¾›äº†é‡è¦ä¾æ®

- äº¤å‰éªŒè¯tæ£€éªŒ(åŸºäºæˆå¯¹tæ£€éªŒ)
  - k-foldäº¤å‰æ£€éªŒ; $5 \times 2$ äº¤å‰éªŒè¯
- McNemaræ£€éªŒ(åŸºäºåˆ—è”è¡¨,å¡æ–¹æ£€éªŒ)







## çº¿æ€§å›å½’(Linear Regression)

#### 1. çº¿æ€§å›å½’

$y \backsimeq \displaystyle \sum_{i=1}^{m} w_ix_i + b = w^Tx+b$

- ç¦»æ•£å˜é‡
  - æœ‰åº: 0,1,2
  - æ— åº: 001,010,100 (kç»´å‘é‡)

ä»¤å‡æ–¹è¯¯å·®æœ€å°åŒ–:

$(w^*,b^*) = \arg\min_\limits{(w,b)} \displaystyle \sum_\limits{i=1}^{m} (f(x_i)-y_i)^2$

$=\arg \min_\limits{(w,b)} \displaystyle \sum_{i=1}^{m} (wx_i + b - y_i)^2$

å¯¹$E(w,b)= \displaystyle \sum_{i=1}^{m}(wx_i+b-y_i)^2$ 

- æœ€å°äºŒä¹˜æ³•ä¼°è®¡

$\cfrac {\partial E(w,b)}{\partial w} = 2 \left( w\displaystyle\sum_{i=1}^{m} x_i^2 - \displaystyle\sum_{i=1}^{m}(y_i -b)x_i \right)$

$\cfrac {\partial E(w,b)}{\partial b} = 2 \left( mb- \displaystyle\sum_{i=1}^{m}(y_i-wx_i) \right)$

ä»¤å¯¼æ•°ä¸º0,å¾—åˆ°é—­å¼(closed-form)è§£:

$w = \cfrac {\displaystyle\sum_{i=1}^{m} y_i(x_i-\overline {x})} {\displaystyle\sum_{i=1}^{m}x_i^2 - \cfrac{1}{m}\left(\displaystyle\sum_{i=1}^{m}x_i \right)^2}$

$b =\cfrac{1}{m}\displaystyle\sum_{i=1}^{m}(y_i -wx_i)$



#### 2. å¤šå…ƒ(multi-variate)çº¿æ€§å›å½’

$y \backsimeq  w_0 + w^1x^1+w^2x^2 +\cdot \cdot \cdot+w^nx^n$

$=\displaystyle \sum_{i=0}^{m}w^{i}x^{i} = W^{T}X$

åé¢æˆ‘ä»¬å¯¹è§‚æµ‹é›†,é‡‡ç”¨ä¸‹é¢è®°å·:

$X_{N \times p} = \begin{bmatrix}
x_{11} & x_{12} & \dots & x_{1p} \\
x_{21} & x_{22} & \dots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{N1} & x_{N2} & \dots & x_{Np}
\end{bmatrix}$

$= (x_1,x_2,x_3,\cdots,x_N)^T$

å…¶ä¸­$x_i = (x_{i1},x_{i2},\cdots,x_{ip})^T \quad (i \in N)$



- æœ€å°äºŒä¹˜ä¼°è®¡

$\hat w^* = \arg\min_\limits{\hat w} (y-X\hat w)^T(y-X\hat w)$

ä»¤$E_{\hat w} = (y-X\hat w)^T(y-X\hat w)$

å¯¹$\hat w$æ±‚å¯¼: $\partial E_{\hat w} = 2X^T(X_{\hat w}-y)$ä»¤å…¶ä¸ºé›¶å¯å¾—$\hat w$ 

- è‹¥$X^TX$ æ»¡ç§©æˆ–æ­£å®š, åˆ™$\hat w^* = (X^TX)^{-1}X^T y$
- è‹¥$X^TX$ä¸æ»¡ç§©, åˆ™å¯ä»¥è§£å‡ºå¤šä¸ª$\hat w$
  - è®¾ç½®å½’çº³åå¥½æˆ–è¡¨è¾¾ä¸ºå¼•å…¥æ­£åˆ™åŒ–



#### 3. å¹¿ä¹‰çº¿æ€§æ¨¡å‹

ä¸€èˆ¬å½¢å¼:
$$
y = g^{-1} (W^TX)
$$

ä¾‹å¦‚å¯¹äº$f(x) = e^{W^TX}$ å¯ä»¥é€šè¿‡å¯¹å…¶æ±‚$\ln$ æ¥é™å¹‚ä»è€Œè¾¾åˆ°çº¿æ€§æ‹Ÿåˆ,å¦‚ä¸‹å›¾

<img src="/imgs/func1.png" />



#### 4. å¯¹ç‡å›å½’:

> å¯¹æ•°å‡ ç‡å›å½’(logistic regression) ç®€ç§°å¯¹ç‡å›å½’
>
> $\cfrac{y}{1-y} \longrightarrow  \cfrac{P(postive |X)}{P(negetive|X)}$ :å‡ ç‡(odds) å³ log odds $\longrightarrow$ logit 

å¯¹äºçº¿æ€§å›å½’æ¨¡å‹äº§ç”Ÿçš„å®å€¼è¾“å‡º$z = W^TX$å’ŒæœŸæœ›è¾“å‡º$y\in \{0,1\}$

ç†æƒ³å‡½æ•° $y(z) = \begin{cases} 0,& z<0 \\ 0.5, &z =0 \\ 1 ,& z>0 \end {cases}$çš„å‡½æ•°æ€§è´¨è¾ƒå·®,å› æ­¤å¯»æ‰¾å¦‚ä¸‹æ›¿ä»£å‡½æ•°$y = \cfrac{1}{1+e^{-z}}$

<img src="/imgs/func.png" />

ç›¸æ¯”ä¹‹ä¸‹,æ›¿ä»£å‡½æ•°çš„æ€§è´¨æ›´å¥½

- å¯¹ç‡å‡½æ•°(logistic function)ä¸é€»è¾‘æ²¡æœ‰ä»»ä½•å…³ç³»
- å®å€¼å‡½æ•°,åœ¨$y \in (0,1)$ è¿ç»­
- ç”¨å›å½’æ¨¡å‹åšåˆ†ç±»

å› æ­¤å¯¹ $y =\cfrac{1}{1+e^{-z}},\quad z =W^TX$

$\Longrightarrow  y = \cfrac{1}{1+e^{-(W^TX)}} \Longrightarrow \ln \cfrac{y}{1-y} = W^TX$

- æ— éœ€äº‹å…ˆè¿›è¡Œå‡è®¾æ•°æ®åˆ†å¸ƒ
- å¯ä»¥å¾—åˆ°â€œç±»åˆ«â€çš„è¿‘ä¼¼æ¦‚ç‡é¢„æµ‹
- å¯ç›´æ¥åº”ç”¨ç°æœ‰æ•°å€¼ä¼˜åŒ–ç®—æ³•æ±‚å–æœ€ä¼˜è§£












- æ¢¯åº¦ä¸‹é™æ³•
  å¦‚æœçŸ©é˜µä¸æ˜¯æ»¡ç§©,æ²¡æœ‰é€†çŸ©é˜µ,å°±æ— æ³•ä½¿ç”¨æœ€å°äºŒä¹˜æ³•







## KNN
â€”â€”Kè¿‘é‚»ç®—æ³•ï¼Œè¡¨ç¤ºä¸€ä¸ªæ ‡æœ¬æ—¶ï¼Œä½¿ç”¨å…¶æœ€æ¥è¿‘çš„Kä¸ªè¿‘é‚»æ¥å†³å®šã€‚

- å¯ä»¥ç”¨äºåˆ†ç±»å’Œå›å½’

##### è¿‡ç¨‹

- ä»è®­ç»ƒé›†ä¸­é€‰æ‹©ç¦»å¾…é¢„æµ‹æ ·æœ¬æœ€è¿‘çš„Kä¸ªæ ·æœ¬
- æ›´å…·Kä¸ªæ ·æœ¬è®¡ç®—å¾…é¢„æµ‹æ ·æœ¬çš„å€¼

##### å®ç°

- æ ¹æ®irisèŠ±æ ·ä¾‹å®ç°, Download from [Kaggle DataSets](https://www.kaggle.com/datasets) æˆ–è€…ä»ç¬”è€…[Github](https://github.com/Chenpeel/Codes/raw/master/Jupyter/ML/files/iris.zip) / [Gitee](https://gitee.com/chenyh43/ML/tree/master/files/iris.zip)

##### Codes

<a href="https://github.com/Chenpeel/Codes/blob/master/Jupyter/ML/KNN.ipynb"> KNN Python </a>

<!-- <a href="./"> KNN C++ </a> -->



